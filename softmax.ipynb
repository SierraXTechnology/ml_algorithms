{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from util import plot_probability_array\n",
    "from util import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair=[1,3] #Sepal Width and Petal Width\n",
    "iris = datasets.load_iris()\n",
    "Features = iris.data[:, pair]\n",
    "Features[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = iris.target\n",
    "np.unique(Target,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Features[:,0], Features[:,1], cmap=plt.cm.RdYlBu, c=Target, s=20, edgecolors='k')\n",
    "plt.xlabel('Sepal Width')\n",
    "plt.ylabel('Petal Width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to logistic regression, we can use the softmax function to predict the probability of each class.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=0).fit(Features, Target)\n",
    "probability = model.predict_proba(Features)\n",
    "#probablity of each sample belonging to a class\n",
    "plot_probability_array(Features,probability)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "print(probability[0:1]) #probabilities of the first sample\n",
    "print(probability[0:1].sum()) #sum of the probabilities of the first sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(probability[0:1]) #predicted class of the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_prediction = np.argmax(probability, axis=1)\n",
    "softmax_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See whats going on under the hood by comparing results\n",
    "Target_ = model.predict(Features)\n",
    "accuracy_score(Target_, softmax_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For One vs All (One-vs-Rest) or One vs One please see the reference \"Multi-class_Classification.ipynb\" in the course dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
